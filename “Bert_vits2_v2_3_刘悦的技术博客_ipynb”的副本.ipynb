{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ideas4u/Bert-VITS2/blob/master/%E2%80%9CBert_vits2_v2_3_%E5%88%98%E6%82%A6%E7%9A%84%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8tQrVHaSe6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de602e1-6336-4e27-8009-db4ec7c6f724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 23 16:28:48 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@title 查看显卡\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 安装ffmpeg\n",
        "import os, uuid, re, IPython\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import output, drive\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import os, sys, urllib.request\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
        "\n",
        "from ttmg import (\n",
        "    loadingAn,\n",
        "    textAn,\n",
        ")\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Cloning Repositories...\", ty='twg')\n",
        "!git clone https://github.com/XniceCraft/ffmpeg-colab.git\n",
        "!chmod 755 ./ffmpeg-colab/install\n",
        "textAn(\"Installing FFmpeg...\", ty='twg')\n",
        "!./ffmpeg-colab/install\n",
        "clear_output()\n",
        "print('Installation finished!')\n",
        "!rm -fr /content/ffmpeg-colab\n",
        "!ffmpeg -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "cellView": "form",
        "id": "XmHuEXvSSw7w",
        "outputId": "6a118fe8-dc2b-4433-a3b8-fbf4d8496197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installation finished!\n",
            "ffmpeg version 6.0 Copyright (c) 2000-2023 the FFmpeg developers\n",
            "built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "configuration: --prefix=/home/ffmpeg-builder/release --pkg-config-flags=--static --extra-libs=-lm --disable-doc --disable-debug --disable-shared --disable-ffprobe --enable-static --enable-gpl --enable-version3 --enable-runtime-cpudetect --enable-avfilter --enable-filters --enable-nvenc --enable-nvdec --enable-cuvid --toolchain=hardened --disable-stripping --enable-opengl --pkgconfigdir=/home/ffmpeg-builder/release/lib/pkgconfig --extra-cflags='-I/home/ffmpeg-builder/release/include -static-libstdc++ -static-libgcc ' --extra-ldflags='-L/home/ffmpeg-builder/release/lib -fstack-protector -static-libstdc++ -static-libgcc ' --extra-cxxflags=' -static-libstdc++ -static-libgcc ' --extra-libs='-ldl -lrt -lpthread' --enable-ffnvcodec --enable-gmp --enable-libaom --enable-libass --enable-libbluray --enable-libdav1d --enable-libfdk-aac --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libkvazaar --enable-libmp3lame --enable-libopus --enable-libopencore_amrnb --enable-libopencore_amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libshine --enable-libsoxr --enable-libsrt --enable-libsvtav1 --enable-libtheora --enable-libvidstab --ld=g++ --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libx264 --enable-libx265 --enable-libxvid --enable-libzimg --enable-openssl --enable-zlib --enable-nonfree --extra-libs=-lpthread --enable-pthreads --extra-libs=-lgomp\n",
            "libavutil      58.  2.100 / 58.  2.100\n",
            "libavcodec     60.  3.100 / 60.  3.100\n",
            "libavformat    60.  3.100 / 60.  3.100\n",
            "libavdevice    60.  1.100 / 60.  1.100\n",
            "libavfilter     9.  3.100 /  9.  3.100\n",
            "libswscale      7.  1.100 /  7.  1.100\n",
            "libswresample   4. 10.100 /  4. 10.100\n",
            "libpostproc    57.  1.100 / 57.  1.100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 克隆代码仓库\n",
        "!git clone https://github.com/v3ucn/Bert-vits2-V2.3.git\n",
        "#%cd /content/Bert-vits2-V2.2\n",
        "#!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCFu_we6TAIU",
        "outputId": "c3d077e5-b888-420a-a014-a904d1dd0498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bert-vits2-V2.3'...\n",
            "remote: Enumerating objects: 234, done.\u001b[K\n",
            "remote: Counting objects: 100% (234/234), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 234 (delta 80), reused 232 (delta 78), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (234/234), 4.16 MiB | 14.14 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 安装所需要的依赖\n",
        "%cd /content/Bert-vits2-V2.3\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoYVxcNpTIP5",
        "outputId": "7baa5ba7-78b6-43c0-9e13-f1a75ae72e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Bert-vits2-V2.3\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 1))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.23.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.58.1)\n",
            "Collecting phonemizer (from -r requirements.txt (line 5))\n",
            "  Downloading phonemizer-3.2.1-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.11.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.15.1)\n",
            "Collecting Unidecode (from -r requirements.txt (line 8))\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting amfm_decompy (from -r requirements.txt (line 9))\n",
            "  Downloading AMFM_decompy-1.0.11.tar.gz (751 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.5/751.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.42.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.35.2)\n",
            "Collecting pypinyin (from -r requirements.txt (line 12))\n",
            "  Downloading pypinyin-0.50.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cn2an (from -r requirements.txt (line 13))\n",
            "  Downloading cn2an-0.5.22-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio==3.50.2 (from -r requirements.txt (line 14))\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av (from -r requirements.txt (line 15))\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mecab-python3 (from -r requirements.txt (line 16))\n",
            "  Downloading mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru (from -r requirements.txt (line 17))\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidic-lite (from -r requirements.txt (line 18))\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cmudict (from -r requirements.txt (line 19))\n",
            "  Downloading cmudict-1.0.16-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fugashi (from -r requirements.txt (line 20))\n",
            "  Downloading fugashi-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (600 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.9/600.9 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting num2words (from -r requirements.txt (line 21))\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (2.31.0)\n",
            "Collecting pyopenjtalk-prebuilt (from -r requirements.txt (line 24))\n",
            "  Downloading pyopenjtalk_prebuilt-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaconv (from -r requirements.txt (line 25))\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (5.9.5)\n",
            "Collecting GPUtil (from -r requirements.txt (line 27))\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting vector_quantize_pytorch (from -r requirements.txt (line 28))\n",
            "  Downloading vector_quantize_pytorch-1.12.5-py3-none-any.whl (24 kB)\n",
            "Collecting g2p_en (from -r requirements.txt (line 29))\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from -r requirements.txt (line 30))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pykakasi (from -r requirements.txt (line 31))\n",
            "  Downloading pykakasi-2.2.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langid (from -r requirements.txt (line 32))\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpeg (from -r requirements.txt (line 33))\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (4.4.2)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.9.2->-r requirements.txt (line 1))\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (23.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (4.2.2)\n",
            "Collecting fastapi (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.1 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (2.1.3)\n",
            "Collecting orjson~=3.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (1.10.13)\n",
            "Collecting pydub (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.1->gradio==3.50.2->-r requirements.txt (line 14)) (2023.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->-r requirements.txt (line 4)) (0.41.1)\n",
            "Collecting segments (from phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.10/dist-packages (from phonemizer->-r requirements.txt (line 5)) (23.1.0)\n",
            "Collecting dlinfo (from phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading dlinfo-1.2.1-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (4.66.1)\n",
            "Collecting proces>=0.1.3 (from cn2an->-r requirements.txt (line 13))\n",
            "  Downloading proces-0.1.7-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->-r requirements.txt (line 19)) (7.0.0)\n",
            "Collecting docopt>=0.6.2 (from num2words->-r requirements.txt (line 21))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (2023.11.17)\n",
            "Requirement already satisfied: cython>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pyopenjtalk-prebuilt->-r requirements.txt (line 24)) (3.0.6)\n",
            "Collecting einops>=0.7.0 (from vector_quantize_pytorch->-r requirements.txt (line 28))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vector_quantize_pytorch->-r requirements.txt (line 28)) (2.1.0+cu121)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p_en->-r requirements.txt (line 29)) (3.8.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en->-r requirements.txt (line 29)) (7.0.0)\n",
            "Collecting distance>=0.1.3 (from g2p_en->-r requirements.txt (line 29))\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from pykakasi->-r requirements.txt (line 31))\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5->cmudict->-r requirements.txt (line 19)) (3.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 29)) (8.1.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.50.2->-r requirements.txt (line 14)) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 1)) (4.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 1)) (1.16.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pykakasi->-r requirements.txt (line 31)) (1.14.1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.50.2->-r requirements.txt (line 14)) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting httpcore==1.* (from httpx->gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->-r requirements.txt (line 14)) (1.3.0)\n",
            "Collecting clldutils>=1.7.3 (from segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading clldutils-3.22.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting csvw>=1.5.6 (from segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading csvw-3.2.1-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (2.1.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.50.2->-r requirements.txt (line 14)) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 1)) (2.21)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5)) (0.9.0)\n",
            "Collecting colorlog (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Collecting bibtexparser>=2.0.0b4 (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading bibtexparser-2.0.0b4-py3-none-any.whl (37 kB)\n",
            "Collecting pylatexenc (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5)) (4.9.3)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5)) (2.14.0)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting language-tags (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdflib (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5)) (4.1.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.15.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (1.3.0)\n",
            "Building wheels for collected packages: amfm_decompy, unidic-lite, jaconv, GPUtil, langid, ffmpeg, distance, docopt, ffmpy, pylatexenc\n",
            "  Building wheel for amfm_decompy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for amfm_decompy: filename=AMFM_decompy-1.0.11-py3-none-any.whl size=42835 sha256=276adfba30515fd7a0b136bbd02f98f1231d14bca93173c53161c516c184bfe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/81/e7/443ad333f2f4ed8c06fc027caeb0d0c84b896fe7e56c2e92b1\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=4f2b8c5e78a00c058e2758fbeb22fb1e9be360afd80e0b7661b883ca8df2750c\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16415 sha256=e26b6e190451dec61d5f57b103bd3df28797b1676e121ebeab8c0a756d210e34\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=1619ee06bf6a47e4dab7431f6a7fabee3a6fa31b57936c10355526cd86a72c76\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=5441d578ee83ae722d100c78d509da439ddc2643b28f048640477faa1ca3c45f\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6080 sha256=26e51e8598c42253f9c4ceee9b55fff48b3b8f496b9cbf367849d1dada7e6df0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=a859de84e1349b7ace72aadbc4f753e4bfd76e912427e65a4d52db0c0ea18eb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=72fb84e28b80dc76e9d81988d80753993afd659124b0603be9d51d6ea1633f9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=7de87a572603a1b4ed3ced5ca3ca5dea2aa9dd24279ac9d34d464f8901b61d4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=42997b7b28455c93e5ca08212922c87dd1b3efa3ebd7fab4292de32bdad3a1a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n",
            "Successfully built amfm_decompy unidic-lite jaconv GPUtil langid ffmpeg distance docopt ffmpy pylatexenc\n",
            "Installing collected packages: unidic-lite, sentencepiece, rfc3986, pylatexenc, pydub, mecab-python3, language-tags, jaconv, GPUtil, ffmpy, ffmpeg, docopt, dlinfo, distance, websockets, Unidecode, typing-extensions, semantic-version, python-multipart, pypinyin, pyopenjtalk-prebuilt, proces, orjson, num2words, loguru, langid, isodate, h11, fugashi, einops, deprecated, colorlog, colorama, bibtexparser, av, aiofiles, uvicorn, starlette, resampy, rdflib, pykakasi, httpcore, cn2an, cmudict, clldutils, amfm_decompy, vector_quantize_pytorch, librosa, httpx, fastapi, gradio-client, g2p_en, csvw, segments, gradio, phonemizer\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.1\n",
            "    Uninstalling librosa-0.10.1:\n",
            "      Successfully uninstalled librosa-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GPUtil-1.4.0 Unidecode-1.3.7 aiofiles-23.2.1 amfm_decompy-1.0.11 av-11.0.0 bibtexparser-2.0.0b4 clldutils-3.22.1 cmudict-1.0.16 cn2an-0.5.22 colorama-0.4.6 colorlog-6.8.0 csvw-3.2.1 deprecated-1.2.14 distance-0.1.3 dlinfo-1.2.1 docopt-0.6.2 einops-0.7.0 fastapi-0.105.0 ffmpeg-1.4 ffmpy-0.3.1 fugashi-1.3.0 g2p_en-2.1.0 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 isodate-0.6.1 jaconv-0.3.4 langid-1.1.6 language-tags-1.2.0 librosa-0.9.2 loguru-0.7.2 mecab-python3-1.0.8 num2words-0.5.13 orjson-3.9.10 phonemizer-3.2.1 proces-0.1.7 pydub-0.25.1 pykakasi-2.2.1 pylatexenc-2.10 pyopenjtalk-prebuilt-0.3.0 pypinyin-0.50.0 python-multipart-0.0.6 rdflib-7.0.0 resampy-0.4.2 rfc3986-1.5.0 segments-2.2.1 semantic-version-2.10.0 sentencepiece-0.1.99 starlette-0.27.0 typing-extensions-4.9.0 unidic-lite-1.0.8 uvicorn-0.25.0 vector_quantize_pytorch-1.12.5 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 下载必要的模型\n",
        "!wget -P slm/wavlm-base-plus/ https://huggingface.co/microsoft/wavlm-base-plus/resolve/main/pytorch_model.bin\n",
        "!wget -P emotional/clap-htsat-fused/ https://huggingface.co/laion/clap-htsat-fused/resolve/main/pytorch_model.bin\n",
        "!wget -P emotional/wav2vec2-large-robust-12-ft-emotion-msp-dim/ https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim/resolve/main/pytorch_model.bin\n",
        "!wget -P bert/chinese-roberta-wwm-ext-large/ https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/pytorch_model.bin\n",
        "!wget -P bert/bert-base-japanese-v3/ https://huggingface.co/cl-tohoku/bert-base-japanese-v3/resolve/main/pytorch_model.bin\n",
        "!wget -P bert/deberta-v3-large/ https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin\n",
        "!wget -P bert/deberta-v3-large/ https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.generator.bin\n",
        "!wget -P bert/deberta-v2-large-japanese/ https://huggingface.co/ku-nlp/deberta-v2-large-japanese/resolve/main/pytorch_model.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJwTPJo_TfRa",
        "outputId": "7834485d-43e7-4230-ff5e-2a049ca2630a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-23 16:30:34--  https://huggingface.co/microsoft/wavlm-base-plus/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.37, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/microsoft/wavlm-base-plus/3bb273a6ace99408b50cfc81afdbb7ef2de02da2eab0234e18db608ce692fe51?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608234&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODIzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9taWNyb3NvZnQvd2F2bG0tYmFzZS1wbHVzLzNiYjI3M2E2YWNlOTk0MDhiNTBjZmM4MWFmZGJiN2VmMmRlMDJkYTJlYWIwMjM0ZTE4ZGI2MDhjZTY5MmZlNTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=PlV4SyYGqPYR3FvvImzE0jABVboso7kBG7aQF-DCnYJqljwR9RMpZXJ%7EWfvdySzNrgE7lNfRbgJ3T3hI-78Mt%7E40DbM%7ELvc5R1Tf-K2tAjso0GyBD-y2WEcR0GRUb3dXiKsQx2VMBQiKJEdqYDN2BS5oddF46nPo4nU8KtcLd8JInKguo7AsZwa1wU0MEjcxbvZHlp3LV-3H6uVSL0qbYR-5gvwpY7mmH2b09oNjOV%7EmQP%7Ear22YWJcRX7MuFiP4pGb084%7EmbnxKR8VIPnJ1phqiuJ0AQhvJ%7Eh39%7EZ9ggCkAS%7EhTR1qWKt4Rg5ra9MScb5Ay0GSD8IByVe2qm4mdnw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-23 16:30:34--  https://cdn-lfs.huggingface.co/microsoft/wavlm-base-plus/3bb273a6ace99408b50cfc81afdbb7ef2de02da2eab0234e18db608ce692fe51?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608234&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODIzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9taWNyb3NvZnQvd2F2bG0tYmFzZS1wbHVzLzNiYjI3M2E2YWNlOTk0MDhiNTBjZmM4MWFmZGJiN2VmMmRlMDJkYTJlYWIwMjM0ZTE4ZGI2MDhjZTY5MmZlNTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=PlV4SyYGqPYR3FvvImzE0jABVboso7kBG7aQF-DCnYJqljwR9RMpZXJ%7EWfvdySzNrgE7lNfRbgJ3T3hI-78Mt%7E40DbM%7ELvc5R1Tf-K2tAjso0GyBD-y2WEcR0GRUb3dXiKsQx2VMBQiKJEdqYDN2BS5oddF46nPo4nU8KtcLd8JInKguo7AsZwa1wU0MEjcxbvZHlp3LV-3H6uVSL0qbYR-5gvwpY7mmH2b09oNjOV%7EmQP%7Ear22YWJcRX7MuFiP4pGb084%7EmbnxKR8VIPnJ1phqiuJ0AQhvJ%7Eh39%7EZ9ggCkAS%7EhTR1qWKt4Rg5ra9MScb5Ay0GSD8IByVe2qm4mdnw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.122, 108.138.94.23, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 377617425 (360M) [application/octet-stream]\n",
            "Saving to: ‘slm/wavlm-base-plus/pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>] 360.12M   289MB/s    in 1.2s    \n",
            "\n",
            "2023-12-23 16:30:36 (289 MB/s) - ‘slm/wavlm-base-plus/pytorch_model.bin’ saved [377617425/377617425]\n",
            "\n",
            "--2023-12-23 16:30:36--  https://huggingface.co/laion/clap-htsat-fused/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.37, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/3c/78/3c786b248e850bf0a53329389f282ce607cda3ac93975fcc817d3042357fe55d/1ed5d0215d887551ddd0a49ce7311b21429ebdf1e6a129d4e68f743357225253?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608236&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODIzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zYy83OC8zYzc4NmIyNDhlODUwYmYwYTUzMzI5Mzg5ZjI4MmNlNjA3Y2RhM2FjOTM5NzVmY2M4MTdkMzA0MjM1N2ZlNTVkLzFlZDVkMDIxNWQ4ODc1NTFkZGQwYTQ5Y2U3MzExYjIxNDI5ZWJkZjFlNmExMjlkNGU2OGY3NDMzNTcyMjUyNTM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=nVaVt-7OOpBu15FjLg0fOm3isi30viPB3JzfH0y2ZxDpZDPCtQp5QjhXjCdO7Xan466NztA4oXjMjJD-UUy07ExDlndi8Opg2dxF9XngWKF28T0gOjt7BMvQauMV3tlvmkg-l0dKCuwy18T4Pee7wLXQ%7Eb1kQ4J%7EEAdorbG9dIfMBt%7EKlP4ULAMr-J7gsvqMx2R1vkKPbZ3%7E1Ls9NhejVPdiZQXyxqTUCoalqkg2J3TgraREfv-LolImIdLwr0KFUeTGIwN%7EJlU6G9X%7EphS0KU2ARgQOGW2aCYEhSCGRD8lqSrVjTGxpa4A2K6qDIjfTsPGx8bQt39AMBTGgXFbwmg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-23 16:30:36--  https://cdn-lfs.huggingface.co/repos/3c/78/3c786b248e850bf0a53329389f282ce607cda3ac93975fcc817d3042357fe55d/1ed5d0215d887551ddd0a49ce7311b21429ebdf1e6a129d4e68f743357225253?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608236&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODIzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zYy83OC8zYzc4NmIyNDhlODUwYmYwYTUzMzI5Mzg5ZjI4MmNlNjA3Y2RhM2FjOTM5NzVmY2M4MTdkMzA0MjM1N2ZlNTVkLzFlZDVkMDIxNWQ4ODc1NTFkZGQwYTQ5Y2U3MzExYjIxNDI5ZWJkZjFlNmExMjlkNGU2OGY3NDMzNTcyMjUyNTM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=nVaVt-7OOpBu15FjLg0fOm3isi30viPB3JzfH0y2ZxDpZDPCtQp5QjhXjCdO7Xan466NztA4oXjMjJD-UUy07ExDlndi8Opg2dxF9XngWKF28T0gOjt7BMvQauMV3tlvmkg-l0dKCuwy18T4Pee7wLXQ%7Eb1kQ4J%7EEAdorbG9dIfMBt%7EKlP4ULAMr-J7gsvqMx2R1vkKPbZ3%7E1Ls9NhejVPdiZQXyxqTUCoalqkg2J3TgraREfv-LolImIdLwr0KFUeTGIwN%7EJlU6G9X%7EphS0KU2ARgQOGW2aCYEhSCGRD8lqSrVjTGxpa4A2K6qDIjfTsPGx8bQt39AMBTGgXFbwmg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.122, 108.138.94.23, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 614596545 (586M) [application/octet-stream]\n",
            "Saving to: ‘emotional/clap-htsat-fused/pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>] 586.12M   284MB/s    in 2.1s    \n",
            "\n",
            "2023-12-23 16:30:38 (284 MB/s) - ‘emotional/clap-htsat-fused/pytorch_model.bin’ saved [614596545/614596545]\n",
            "\n",
            "--2023-12-23 16:30:38--  https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.37, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/bc/c0/bcc0d7592571998e2661b0cfbc9d355cde2bf9c339c545de32350cb8861df10f/176d9d1ce29a8bddbab44068b9c1c194c51624c7f1812905e01355da58b18816?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608238&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODIzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iYy9jMC9iY2MwZDc1OTI1NzE5OThlMjY2MWIwY2ZiYzlkMzU1Y2RlMmJmOWMzMzljNTQ1ZGUzMjM1MGNiODg2MWRmMTBmLzE3NmQ5ZDFjZTI5YThiZGRiYWI0NDA2OGI5YzFjMTk0YzUxNjI0YzdmMTgxMjkwNWUwMTM1NWRhNThiMTg4MTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=0gpI%7E0uAIwn0msLYaV-hM9qMdp%7Emn5pAW19nIBvIRdFuIxazckDiW0dnKCkmXYSGxConw8p-lgyP2g4PqJkYg7-jI0QsFAoTpis7jHUAtKfnDLcoGUheVZxrO-2O228pb1ovSeIaQ0%7E21vRGNJwLlpHLJYj-Mp48HRlhVHafE43EwAKQFZo5aF6xHE30OwnDWjY8euihmFuvg-foWyRZU-0xMtzoKvnHEId4ZunMnH-5Hl8-m3so64nU8FuLW6SpKew1vCtJQdOIGLfhk3JR9W4cjapAV%7EarRRyJ3h00Zwb1V%7EcT07PdlgceL8F3DbFmcCp1BjILHICDlq3TG8TWYw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-23 16:30:38--  https://cdn-lfs.huggingface.co/repos/bc/c0/bcc0d7592571998e2661b0cfbc9d355cde2bf9c339c545de32350cb8861df10f/176d9d1ce29a8bddbab44068b9c1c194c51624c7f1812905e01355da58b18816?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608238&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODIzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iYy9jMC9iY2MwZDc1OTI1NzE5OThlMjY2MWIwY2ZiYzlkMzU1Y2RlMmJmOWMzMzljNTQ1ZGUzMjM1MGNiODg2MWRmMTBmLzE3NmQ5ZDFjZTI5YThiZGRiYWI0NDA2OGI5YzFjMTk0YzUxNjI0YzdmMTgxMjkwNWUwMTM1NWRhNThiMTg4MTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=0gpI%7E0uAIwn0msLYaV-hM9qMdp%7Emn5pAW19nIBvIRdFuIxazckDiW0dnKCkmXYSGxConw8p-lgyP2g4PqJkYg7-jI0QsFAoTpis7jHUAtKfnDLcoGUheVZxrO-2O228pb1ovSeIaQ0%7E21vRGNJwLlpHLJYj-Mp48HRlhVHafE43EwAKQFZo5aF6xHE30OwnDWjY8euihmFuvg-foWyRZU-0xMtzoKvnHEId4ZunMnH-5Hl8-m3so64nU8FuLW6SpKew1vCtJQdOIGLfhk3JR9W4cjapAV%7EarRRyJ3h00Zwb1V%7EcT07PdlgceL8F3DbFmcCp1BjILHICDlq3TG8TWYw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.122, 108.138.94.23, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 661436013 (631M) [application/octet-stream]\n",
            "Saving to: ‘emotional/wav2vec2-large-robust-12-ft-emotion-msp-dim/pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>] 630.79M   263MB/s    in 2.4s    \n",
            "\n",
            "2023-12-23 16:30:41 (263 MB/s) - ‘emotional/wav2vec2-large-robust-12-ft-emotion-msp-dim/pytorch_model.bin’ saved [661436013/661436013]\n",
            "\n",
            "--2023-12-23 16:30:41--  https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.37, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/hfl/chinese-roberta-wwm-ext-large/4ac62d49144d770c5ca9a5d1d3039c4995665a080febe63198189857c6bd11cd?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608241&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9oZmwvY2hpbmVzZS1yb2JlcnRhLXd3bS1leHQtbGFyZ2UvNGFjNjJkNDkxNDRkNzcwYzVjYTlhNWQxZDMwMzljNDk5NTY2NWEwODBmZWJlNjMxOTgxODk4NTdjNmJkMTFjZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=m-BW%7EDwsIafpuiOd4Fd8SyzBgdgZhzXytgGnLQV41whhNmOkx6tpw2o3eQE2qrFiD9dwzXiu470Ybc-2hkdArsDgIjwrklA151U%7EAoV8z526G1j3VHrEcDDDhJRNRDMxmV7xCARYMZnGpXX%7EclKPYmgQuA-mr7ehCzfyn2IrSbeugeYHbyi-cfXktm%7EtgZtF8oDnknAit0x5xwFmPyhIVZ7n8KrQw71YwUEt7KSdPw7QtdGU7JwBiCxgcwmCHghm-A36DYOIp%7E3kJKYChrItAhuKmAbBlPxq2PZ8ZPl9DuvgPV9N6OWBNnIaKr86OOQHh3yG7K6jjHG0Aa0Qzk%7E8oQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-23 16:30:41--  https://cdn-lfs.huggingface.co/hfl/chinese-roberta-wwm-ext-large/4ac62d49144d770c5ca9a5d1d3039c4995665a080febe63198189857c6bd11cd?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608241&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9oZmwvY2hpbmVzZS1yb2JlcnRhLXd3bS1leHQtbGFyZ2UvNGFjNjJkNDkxNDRkNzcwYzVjYTlhNWQxZDMwMzljNDk5NTY2NWEwODBmZWJlNjMxOTgxODk4NTdjNmJkMTFjZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=m-BW%7EDwsIafpuiOd4Fd8SyzBgdgZhzXytgGnLQV41whhNmOkx6tpw2o3eQE2qrFiD9dwzXiu470Ybc-2hkdArsDgIjwrklA151U%7EAoV8z526G1j3VHrEcDDDhJRNRDMxmV7xCARYMZnGpXX%7EclKPYmgQuA-mr7ehCzfyn2IrSbeugeYHbyi-cfXktm%7EtgZtF8oDnknAit0x5xwFmPyhIVZ7n8KrQw71YwUEt7KSdPw7QtdGU7JwBiCxgcwmCHghm-A36DYOIp%7E3kJKYChrItAhuKmAbBlPxq2PZ8ZPl9DuvgPV9N6OWBNnIaKr86OOQHh3yG7K6jjHG0Aa0Qzk%7E8oQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.122, 108.138.94.23, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1306484351 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘bert/chinese-roberta-wwm-ext-large/pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>]   1.22G   202MB/s    in 6.1s    \n",
            "\n",
            "2023-12-23 16:30:47 (204 MB/s) - ‘bert/chinese-roberta-wwm-ext-large/pytorch_model.bin’ saved [1306484351/1306484351]\n",
            "\n",
            "--2023-12-23 16:30:47--  https://huggingface.co/cl-tohoku/bert-base-japanese-v3/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.37, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/4c/88/4c88375ccabf6c7d262df5cfed1e7bcdcb362ef06f50a3a62e381bc3bf3f6bc6/e172862e0674054d65e0ba40d67df2a4687982f589db44aa27091c386e5450a4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703602402&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwMjQwMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Yy84OC80Yzg4Mzc1Y2NhYmY2YzdkMjYyZGY1Y2ZlZDFlN2JjZGNiMzYyZWYwNmY1MGEzYTYyZTM4MWJjM2JmM2Y2YmM2L2UxNzI4NjJlMDY3NDA1NGQ2NWUwYmE0MGQ2N2RmMmE0Njg3OTgyZjU4OWRiNDRhYTI3MDkxYzM4NmU1NDUwYTQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=nAHK5PZFfBIqUDVywcRaJBtLTmg1qHpBI7aQAlnbD62WeeIs5WPJdKTNiM0AB3N299s-7TJBdbgpwKledFLYznEKoiDAvTqYRa%7EWHtNpISjmuyIMWZcGUaIDPNI--Btry8X4Fpu3rRsbfOicP4oljjHXhOZ3hL2x0-mB0S6Ma-7lwBXA1VI13LnOIINMXo-6lVkMKPbv4OYZOOffG5lzKaJ6EtiqKFUU4MEx5eLvHG3UkTWYno0-u0Wjf1dbhi5qXHHrFg8HnS8SZ4%7E44phhdTjGuuH3d9RAntXA94K%7EaiHqeV5%7Ek4vUmc9w6sGNGacGli9Z1DnRCl8Lj6X5y2YPMg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-23 16:30:47--  https://cdn-lfs.huggingface.co/repos/4c/88/4c88375ccabf6c7d262df5cfed1e7bcdcb362ef06f50a3a62e381bc3bf3f6bc6/e172862e0674054d65e0ba40d67df2a4687982f589db44aa27091c386e5450a4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703602402&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwMjQwMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Yy84OC80Yzg4Mzc1Y2NhYmY2YzdkMjYyZGY1Y2ZlZDFlN2JjZGNiMzYyZWYwNmY1MGEzYTYyZTM4MWJjM2JmM2Y2YmM2L2UxNzI4NjJlMDY3NDA1NGQ2NWUwYmE0MGQ2N2RmMmE0Njg3OTgyZjU4OWRiNDRhYTI3MDkxYzM4NmU1NDUwYTQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=nAHK5PZFfBIqUDVywcRaJBtLTmg1qHpBI7aQAlnbD62WeeIs5WPJdKTNiM0AB3N299s-7TJBdbgpwKledFLYznEKoiDAvTqYRa%7EWHtNpISjmuyIMWZcGUaIDPNI--Btry8X4Fpu3rRsbfOicP4oljjHXhOZ3hL2x0-mB0S6Ma-7lwBXA1VI13LnOIINMXo-6lVkMKPbv4OYZOOffG5lzKaJ6EtiqKFUU4MEx5eLvHG3UkTWYno0-u0Wjf1dbhi5qXHHrFg8HnS8SZ4%7E44phhdTjGuuH3d9RAntXA94K%7EaiHqeV5%7Ek4vUmc9w6sGNGacGli9Z1DnRCl8Lj6X5y2YPMg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.122, 108.138.94.23, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 447406217 (427M) [application/octet-stream]\n",
            "Saving to: ‘bert/bert-base-japanese-v3/pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>] 426.68M   277MB/s    in 1.5s    \n",
            "\n",
            "2023-12-23 16:30:49 (277 MB/s) - ‘bert/bert-base-japanese-v3/pytorch_model.bin’ saved [447406217/447406217]\n",
            "\n",
            "--2023-12-23 16:30:49--  https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.37, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/microsoft/deberta-v3-large/dd5b5d93e2db101aaf281df0ea1216c07ad73620ff59c5b42dccac4bf2eef5b5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608250&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9taWNyb3NvZnQvZGViZXJ0YS12My1sYXJnZS9kZDViNWQ5M2UyZGIxMDFhYWYyODFkZjBlYTEyMTZjMDdhZDczNjIwZmY1OWM1YjQyZGNjYWM0YmYyZWVmNWI1P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=DPKRkD5LjO0VEr-rEqfNSPYIoq1cNcXKlnNI2U10w1UwojPoRn0YQKodTXzVIvMz-NMkWA4Q4p6lYyRxJh1GgXMAdIzMGoL5n8Kr3aCSuy09Tf80Q2hPm6UUS2N%7EbyB4wQn60x28OW16b0IhsmPmSRbV9Ra8x9tieX-BReI0PN26cgicvvmyXIEymxMjTbOSA0aoSusptuf6p%7ECHnUlEvMB04jlwaeCeL8Lnq3BY-z2GySOxC8Njeoy-W1av2xYNuimV57cePhDj26U5uLXIZJWF03iYBFc8S4H2mzGQiWxiz2%7EbKbc9QGToZoix8VqOWkTtM%7Em7i4MRtJskr6JFVA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-23 16:30:50--  https://cdn-lfs.huggingface.co/microsoft/deberta-v3-large/dd5b5d93e2db101aaf281df0ea1216c07ad73620ff59c5b42dccac4bf2eef5b5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608250&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9taWNyb3NvZnQvZGViZXJ0YS12My1sYXJnZS9kZDViNWQ5M2UyZGIxMDFhYWYyODFkZjBlYTEyMTZjMDdhZDczNjIwZmY1OWM1YjQyZGNjYWM0YmYyZWVmNWI1P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=DPKRkD5LjO0VEr-rEqfNSPYIoq1cNcXKlnNI2U10w1UwojPoRn0YQKodTXzVIvMz-NMkWA4Q4p6lYyRxJh1GgXMAdIzMGoL5n8Kr3aCSuy09Tf80Q2hPm6UUS2N%7EbyB4wQn60x28OW16b0IhsmPmSRbV9Ra8x9tieX-BReI0PN26cgicvvmyXIEymxMjTbOSA0aoSusptuf6p%7ECHnUlEvMB04jlwaeCeL8Lnq3BY-z2GySOxC8Njeoy-W1av2xYNuimV57cePhDj26U5uLXIZJWF03iYBFc8S4H2mzGQiWxiz2%7EbKbc9QGToZoix8VqOWkTtM%7Em7i4MRtJskr6JFVA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.122, 108.138.94.23, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 873673253 (833M) [application/octet-stream]\n",
            "Saving to: ‘bert/deberta-v3-large/pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>] 833.20M   271MB/s    in 3.1s    \n",
            "\n",
            "2023-12-23 16:30:53 (271 MB/s) - ‘bert/deberta-v3-large/pytorch_model.bin’ saved [873673253/873673253]\n",
            "\n",
            "--2023-12-23 16:30:53--  https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.generator.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.37, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/microsoft/deberta-v3-large/ff85455c562822ea7001f810d026a68da8a24ffdae5a095081dfe7e84e27989d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.generator.bin%3B+filename%3D%22pytorch_model.generator.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608253&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9taWNyb3NvZnQvZGViZXJ0YS12My1sYXJnZS9mZjg1NDU1YzU2MjgyMmVhNzAwMWY4MTBkMDI2YTY4ZGE4YTI0ZmZkYWU1YTA5NTA4MWRmZTdlODRlMjc5ODlkP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=UNyCudOl%7EdPh0vdOHxsuSv3jzmIUekc-CGlu9kjhNQcNzN6wY-3lskNqtLKixsBrCNWMaWWfnpBpk5-ibejMjhNKMKTvjLYY0CrK9HLgZN%7E2ohXAW0HGmHMxW-Lp2YDmsxfYGiu9ZIDWD2e2vupYwagA2m35R3J87bf%7El0PBX1h%7ED0Z1P-J5Pkx8rjaAiJ8tFEdEUq1tDlfcLevvTSO8w8FPFo6zbWUTmgEn7%7EDzRTHmSE82d5dxb2fkjxBQR03934DUJ7qNIG1TiMERVAtYUY%7E1Psgit2C1tewltZ%7EiMBwvPuvOpTWOWgo6zN6DCHvKNOZvy4STyfRszU-NtlpTkQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-23 16:30:53--  https://cdn-lfs.huggingface.co/microsoft/deberta-v3-large/ff85455c562822ea7001f810d026a68da8a24ffdae5a095081dfe7e84e27989d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.generator.bin%3B+filename%3D%22pytorch_model.generator.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608253&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9taWNyb3NvZnQvZGViZXJ0YS12My1sYXJnZS9mZjg1NDU1YzU2MjgyMmVhNzAwMWY4MTBkMDI2YTY4ZGE4YTI0ZmZkYWU1YTA5NTA4MWRmZTdlODRlMjc5ODlkP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=UNyCudOl%7EdPh0vdOHxsuSv3jzmIUekc-CGlu9kjhNQcNzN6wY-3lskNqtLKixsBrCNWMaWWfnpBpk5-ibejMjhNKMKTvjLYY0CrK9HLgZN%7E2ohXAW0HGmHMxW-Lp2YDmsxfYGiu9ZIDWD2e2vupYwagA2m35R3J87bf%7El0PBX1h%7ED0Z1P-J5Pkx8rjaAiJ8tFEdEUq1tDlfcLevvTSO8w8FPFo6zbWUTmgEn7%7EDzRTHmSE82d5dxb2fkjxBQR03934DUJ7qNIG1TiMERVAtYUY%7E1Psgit2C1tewltZ%7EiMBwvPuvOpTWOWgo6zN6DCHvKNOZvy4STyfRszU-NtlpTkQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.122, 108.138.94.23, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 571293153 (545M) [application/octet-stream]\n",
            "Saving to: ‘bert/deberta-v3-large/pytorch_model.generator.bin’\n",
            "\n",
            "pytorch_model.gener 100%[===================>] 544.83M   239MB/s    in 2.3s    \n",
            "\n",
            "2023-12-23 16:30:55 (239 MB/s) - ‘bert/deberta-v3-large/pytorch_model.generator.bin’ saved [571293153/571293153]\n",
            "\n",
            "--2023-12-23 16:30:55--  https://huggingface.co/ku-nlp/deberta-v2-large-japanese/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.37, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/fb/74/fb74fb75074d6423a21a0caa0fd656e7e4f1f667b506d6ddcb7305046f5feb01/a6c15feac0dea77ab8835c70e1befa4cf4c2137862c6fb2443b1553f70840047?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608255&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mYi83NC9mYjc0ZmI3NTA3NGQ2NDIzYTIxYTBjYWEwZmQ2NTZlN2U0ZjFmNjY3YjUwNmQ2ZGRjYjczMDUwNDZmNWZlYjAxL2E2YzE1ZmVhYzBkZWE3N2FiODgzNWM3MGUxYmVmYTRjZjRjMjEzNzg2MmM2ZmIyNDQzYjE1NTNmNzA4NDAwNDc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=QAv42ZJ7sKEeK2hsPfU9%7ElMUVsRnc6nOsZH%7Ejvx3NbfWjxHLfc9L8c2qTcOAEWjPXFZ2JmWFri9KoOOvGPrci1KnEvEb6glelJTzlYFcs9d-7MmTgNQsDS1-DKIo4JDedM%7EBvmqNn5p1bzjVfaaLZTgpUDKpRFSDtV3o6NfoBvw02fMkQ4JMSeUMepEyaWHBKJkJU6rrZVsI3zs%7EToP-qjyK8Uz-I1Re6QiliNO4mUIdPxiF1Mra6hbQ0mtIi5A7-k5AByTliu3-Bid8yzBwVr3dopQUqOIJYDjLH8gb0niZQp3tG-sMlVzEe6yBmh0lRG0O7MjF2onqAp2Sas3kGQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-23 16:30:55--  https://cdn-lfs.huggingface.co/repos/fb/74/fb74fb75074d6423a21a0caa0fd656e7e4f1f667b506d6ddcb7305046f5feb01/a6c15feac0dea77ab8835c70e1befa4cf4c2137862c6fb2443b1553f70840047?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1703608255&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mYi83NC9mYjc0ZmI3NTA3NGQ2NDIzYTIxYTBjYWEwZmQ2NTZlN2U0ZjFmNjY3YjUwNmQ2ZGRjYjczMDUwNDZmNWZlYjAxL2E2YzE1ZmVhYzBkZWE3N2FiODgzNWM3MGUxYmVmYTRjZjRjMjEzNzg2MmM2ZmIyNDQzYjE1NTNmNzA4NDAwNDc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=QAv42ZJ7sKEeK2hsPfU9%7ElMUVsRnc6nOsZH%7Ejvx3NbfWjxHLfc9L8c2qTcOAEWjPXFZ2JmWFri9KoOOvGPrci1KnEvEb6glelJTzlYFcs9d-7MmTgNQsDS1-DKIo4JDedM%7EBvmqNn5p1bzjVfaaLZTgpUDKpRFSDtV3o6NfoBvw02fMkQ4JMSeUMepEyaWHBKJkJU6rrZVsI3zs%7EToP-qjyK8Uz-I1Re6QiliNO4mUIdPxiF1Mra6hbQ0mtIi5A7-k5AByTliu3-Bid8yzBwVr3dopQUqOIJYDjLH8gb0niZQp3tG-sMlVzEe6yBmh0lRG0O7MjF2onqAp2Sas3kGQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.122, 108.138.94.23, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1490693213 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘bert/deberta-v2-large-japanese/pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>]   1.39G   259MB/s    in 6.0s    \n",
            "\n",
            "2023-12-23 16:31:01 (238 MB/s) - ‘bert/deberta-v2-large-japanese/pytorch_model.bin’ saved [1490693213/1490693213]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 下载底模文件\n",
        "\n",
        "!wget -P Data/ada/models/ https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/DUR_0.pth\n",
        "!wget -P Data/ada/models/ https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/D_0.pth\n",
        "!wget -P Data/ada/models/ https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/G_0.pth\n",
        "!wget -P Data/ada/models/ https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/WD_0.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5lOZ-DwVQcx",
        "outputId": "33c625f0-5d1b-431b-bd1a-408c04ef0fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-23 16:31:09--  https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/DUR_0.pth\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.37, 3.163.189.74, 3.163.189.90, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/b9/bc/b9bca306b06adc45af44c9fb34e0d15da0afb8fa947a3240447868a7b984f4c1/b632d0701be898e0ce8737a055c2171e9afc1327aa3faa855c810076d9da73e7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27DUR_0.pth%3B+filename%3D%22DUR_0.pth%22%3B&Expires=1703608269&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I5L2JjL2I5YmNhMzA2YjA2YWRjNDVhZjQ0YzlmYjM0ZTBkMTVkYTBhZmI4ZmE5NDdhMzI0MDQ0Nzg2OGE3Yjk4NGY0YzEvYjYzMmQwNzAxYmU4OThlMGNlODczN2EwNTVjMjE3MWU5YWZjMTMyN2FhM2ZhYTg1NWM4MTAwNzZkOWRhNzNlNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Cj3XVMxWibjAgX0CKIkBtEhZJprCYo26CDdbhHqfuDZ5DGV7IuWvLfAwuUUooNidtEB0sUXXX6OrODGoFAPcY%7EnZ-uHuYCgD34lEDmvLL9XNRfsMp4MoOLzyLuIL-2ecIWyxjsETc17KlOG4AW-QzsFKCe0bHINZU8-b4aJtoqU8pfXdGEY0bunCXcgjoBJ4ObERIkS-yZoLWLvmwNMs7KEFw2M3ok5Lpw9n7GXGhA7lvvK42ZsOYlB-8Baqthq8sKruE9wKf-wvZmJHGNK4VI9CmV2AJfDcYjs8kwWQToArL01WOE%7EZYqwglUMoqgq%7EMz1NbWAekpB1CvXyyKpefw__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2023-12-23 16:31:09--  https://cdn-lfs-us-1.huggingface.co/repos/b9/bc/b9bca306b06adc45af44c9fb34e0d15da0afb8fa947a3240447868a7b984f4c1/b632d0701be898e0ce8737a055c2171e9afc1327aa3faa855c810076d9da73e7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27DUR_0.pth%3B+filename%3D%22DUR_0.pth%22%3B&Expires=1703608269&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I5L2JjL2I5YmNhMzA2YjA2YWRjNDVhZjQ0YzlmYjM0ZTBkMTVkYTBhZmI4ZmE5NDdhMzI0MDQ0Nzg2OGE3Yjk4NGY0YzEvYjYzMmQwNzAxYmU4OThlMGNlODczN2EwNTVjMjE3MWU5YWZjMTMyN2FhM2ZhYTg1NWM4MTAwNzZkOWRhNzNlNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Cj3XVMxWibjAgX0CKIkBtEhZJprCYo26CDdbhHqfuDZ5DGV7IuWvLfAwuUUooNidtEB0sUXXX6OrODGoFAPcY%7EnZ-uHuYCgD34lEDmvLL9XNRfsMp4MoOLzyLuIL-2ecIWyxjsETc17KlOG4AW-QzsFKCe0bHINZU8-b4aJtoqU8pfXdGEY0bunCXcgjoBJ4ObERIkS-yZoLWLvmwNMs7KEFw2M3ok5Lpw9n7GXGhA7lvvK42ZsOYlB-8Baqthq8sKruE9wKf-wvZmJHGNK4VI9CmV2AJfDcYjs8kwWQToArL01WOE%7EZYqwglUMoqgq%7EMz1NbWAekpB1CvXyyKpefw__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.163.189.20, 3.163.189.28, 3.163.189.91, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.163.189.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14533561 (14M) [binary/octet-stream]\n",
            "Saving to: ‘Data/ada/models/DUR_0.pth’\n",
            "\n",
            "DUR_0.pth           100%[===================>]  13.86M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-12-23 16:31:09 (176 MB/s) - ‘Data/ada/models/DUR_0.pth’ saved [14533561/14533561]\n",
            "\n",
            "--2023-12-23 16:31:09--  https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/D_0.pth\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.37, 3.163.189.74, 3.163.189.90, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/b9/bc/b9bca306b06adc45af44c9fb34e0d15da0afb8fa947a3240447868a7b984f4c1/ad31b5886cd4946f96818691ab06cc3f3ebbeb37d1ddffdd952557f0651c29ec?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27D_0.pth%3B+filename%3D%22D_0.pth%22%3B&Expires=1703608269&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I5L2JjL2I5YmNhMzA2YjA2YWRjNDVhZjQ0YzlmYjM0ZTBkMTVkYTBhZmI4ZmE5NDdhMzI0MDQ0Nzg2OGE3Yjk4NGY0YzEvYWQzMWI1ODg2Y2Q0OTQ2Zjk2ODE4NjkxYWIwNmNjM2YzZWJiZWIzN2QxZGRmZmRkOTUyNTU3ZjA2NTFjMjllYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Vq%7Ey%7Ew8KDVmkaa0g3FDi0lEUQXrXbH%7EdRL1D0e3RJNRgzxAdwKwxpVcUuYESjMIWjwXatije7hvECqoClxdrAzZvY3Pqbco%7EeoH8vZNx8Izt1l0OfXkAJWuHalPZ6f2eCdewyJAW6sWMl-kSO0fi-L2WNOUmzpmKbI5bz5Ug3R5i0eOaVWSPOZdb3JIaqEqsbTGn-6XMCUxUHR6TvCFZQzsVJU-cJKo0Q5CBsBcxK1tUFIkDdlXV7lHiPqTyQSEvmmlNVpzSeS-TJXxhG4hugEjV%7EEmPvfgShgM5lzviWmB%7E7PahbZRI5gGyjMfvTLJdFSwrDIzM8sT8aY5hIFN-Zg__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2023-12-23 16:31:09--  https://cdn-lfs-us-1.huggingface.co/repos/b9/bc/b9bca306b06adc45af44c9fb34e0d15da0afb8fa947a3240447868a7b984f4c1/ad31b5886cd4946f96818691ab06cc3f3ebbeb37d1ddffdd952557f0651c29ec?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27D_0.pth%3B+filename%3D%22D_0.pth%22%3B&Expires=1703608269&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I5L2JjL2I5YmNhMzA2YjA2YWRjNDVhZjQ0YzlmYjM0ZTBkMTVkYTBhZmI4ZmE5NDdhMzI0MDQ0Nzg2OGE3Yjk4NGY0YzEvYWQzMWI1ODg2Y2Q0OTQ2Zjk2ODE4NjkxYWIwNmNjM2YzZWJiZWIzN2QxZGRmZmRkOTUyNTU3ZjA2NTFjMjllYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Vq%7Ey%7Ew8KDVmkaa0g3FDi0lEUQXrXbH%7EdRL1D0e3RJNRgzxAdwKwxpVcUuYESjMIWjwXatije7hvECqoClxdrAzZvY3Pqbco%7EeoH8vZNx8Izt1l0OfXkAJWuHalPZ6f2eCdewyJAW6sWMl-kSO0fi-L2WNOUmzpmKbI5bz5Ug3R5i0eOaVWSPOZdb3JIaqEqsbTGn-6XMCUxUHR6TvCFZQzsVJU-cJKo0Q5CBsBcxK1tUFIkDdlXV7lHiPqTyQSEvmmlNVpzSeS-TJXxhG4hugEjV%7EEmPvfgShgM5lzviWmB%7E7PahbZRI5gGyjMfvTLJdFSwrDIzM8sT8aY5hIFN-Zg__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.163.189.20, 3.163.189.28, 3.163.189.91, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.163.189.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 561078330 (535M) [binary/octet-stream]\n",
            "Saving to: ‘Data/ada/models/D_0.pth’\n",
            "\n",
            "D_0.pth             100%[===================>] 535.09M  46.3MB/s    in 4.9s    \n",
            "\n",
            "2023-12-23 16:31:14 (110 MB/s) - ‘Data/ada/models/D_0.pth’ saved [561078330/561078330]\n",
            "\n",
            "--2023-12-23 16:31:14--  https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/G_0.pth\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.37, 3.163.189.74, 3.163.189.90, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/b9/bc/b9bca306b06adc45af44c9fb34e0d15da0afb8fa947a3240447868a7b984f4c1/b1ab38510fcad0602ea7ead17e47d4677545fb9c510a2d0596c961699949e0ed?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27G_0.pth%3B+filename%3D%22G_0.pth%22%3B&Expires=1703608275&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I5L2JjL2I5YmNhMzA2YjA2YWRjNDVhZjQ0YzlmYjM0ZTBkMTVkYTBhZmI4ZmE5NDdhMzI0MDQ0Nzg2OGE3Yjk4NGY0YzEvYjFhYjM4NTEwZmNhZDA2MDJlYTdlYWQxN2U0N2Q0Njc3NTQ1ZmI5YzUxMGEyZDA1OTZjOTYxNjk5OTQ5ZTBlZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=hplt7QtmneSFgx478ZnzuQoAKM3NhwNGxGAnKaqfnviIrphprp7-cFxpAw7NN79PwaUxmRlJrQ1zAYkenJ-WD2UsIG1TV0H1BDo6CxFRPQ8t1P5EefN4xmLD0QLSe9haKwiddmy4JMoel2QU3Y9pV7m83waXRlngAoQhB8ZDHDkQXF%7EolE8rkPyiZXR3klW7gUOXqy-qYE8RFpT169T4H7q3WOn1mBG277MRvLx9NG-cnkMJrVA1OfRa9o1ikYDSXPT5iGOdnps0ZLvu5LQGosi0CxgNGdpChiUDlzV0OKeeUAMoXKSsBHjJ7UBVv9p9dLMuoFyuasf4GifQoSySbA__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2023-12-23 16:31:15--  https://cdn-lfs-us-1.huggingface.co/repos/b9/bc/b9bca306b06adc45af44c9fb34e0d15da0afb8fa947a3240447868a7b984f4c1/b1ab38510fcad0602ea7ead17e47d4677545fb9c510a2d0596c961699949e0ed?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27G_0.pth%3B+filename%3D%22G_0.pth%22%3B&Expires=1703608275&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I5L2JjL2I5YmNhMzA2YjA2YWRjNDVhZjQ0YzlmYjM0ZTBkMTVkYTBhZmI4ZmE5NDdhMzI0MDQ0Nzg2OGE3Yjk4NGY0YzEvYjFhYjM4NTEwZmNhZDA2MDJlYTdlYWQxN2U0N2Q0Njc3NTQ1ZmI5YzUxMGEyZDA1OTZjOTYxNjk5OTQ5ZTBlZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=hplt7QtmneSFgx478ZnzuQoAKM3NhwNGxGAnKaqfnviIrphprp7-cFxpAw7NN79PwaUxmRlJrQ1zAYkenJ-WD2UsIG1TV0H1BDo6CxFRPQ8t1P5EefN4xmLD0QLSe9haKwiddmy4JMoel2QU3Y9pV7m83waXRlngAoQhB8ZDHDkQXF%7EolE8rkPyiZXR3klW7gUOXqy-qYE8RFpT169T4H7q3WOn1mBG277MRvLx9NG-cnkMJrVA1OfRa9o1ikYDSXPT5iGOdnps0ZLvu5LQGosi0CxgNGdpChiUDlzV0OKeeUAMoXKSsBHjJ7UBVv9p9dLMuoFyuasf4GifQoSySbA__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.163.189.20, 3.163.189.28, 3.163.189.91, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.163.189.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 731623963 (698M) [binary/octet-stream]\n",
            "Saving to: ‘Data/ada/models/G_0.pth’\n",
            "\n",
            "G_0.pth             100%[===================>] 697.73M   271MB/s    in 2.6s    \n",
            "\n",
            "2023-12-23 16:31:17 (271 MB/s) - ‘Data/ada/models/G_0.pth’ saved [731623963/731623963]\n",
            "\n",
            "--2023-12-23 16:31:17--  https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/WD_0.pth\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.37, 3.163.189.74, 3.163.189.90, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/b9/bc/b9bca306b06adc45af44c9fb34e0d15da0afb8fa947a3240447868a7b984f4c1/fbf240734fced2cad8a537cac78d6759894436d1f12a107949eca0c430620907?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27WD_0.pth%3B+filename%3D%22WD_0.pth%22%3B&Expires=1703608277&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI3N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I5L2JjL2I5YmNhMzA2YjA2YWRjNDVhZjQ0YzlmYjM0ZTBkMTVkYTBhZmI4ZmE5NDdhMzI0MDQ0Nzg2OGE3Yjk4NGY0YzEvZmJmMjQwNzM0ZmNlZDJjYWQ4YTUzN2NhYzc4ZDY3NTk4OTQ0MzZkMWYxMmExMDc5NDllY2EwYzQzMDYyMDkwNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=PKpyRjHbuN-CdkAx6%7E3vwgDYgA0-AycFEc0TJY3-q06iIcNSDoKTWHRvweUpmYJPsHXY1EgXAw1bP04MQ-ZcTdQVNjRtyOXDsnlOCKEqZEi2TToO6voDOOY%7EEmcYASYM2DyYaVwbb0WVJm7RXtbfHPwgWBfPcG1Ymvh6w9oJPdDSvKafgxR%7EzNVNew1Uj3BoqvdnLoZsVKjPnA3oYAAgjAobinxbgjKS9OXU5tJMpw4KewZikrOCbu6I5U9oB%7EI5vZgokRNsSDOloZVapmQXO7KbLoiQCsmQw4UIUMqror2d0ms46d8B9eMUKe7l2RcyayE3xWJGBztrJaXOniguew__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2023-12-23 16:31:17--  https://cdn-lfs-us-1.huggingface.co/repos/b9/bc/b9bca306b06adc45af44c9fb34e0d15da0afb8fa947a3240447868a7b984f4c1/fbf240734fced2cad8a537cac78d6759894436d1f12a107949eca0c430620907?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27WD_0.pth%3B+filename%3D%22WD_0.pth%22%3B&Expires=1703608277&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzYwODI3N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I5L2JjL2I5YmNhMzA2YjA2YWRjNDVhZjQ0YzlmYjM0ZTBkMTVkYTBhZmI4ZmE5NDdhMzI0MDQ0Nzg2OGE3Yjk4NGY0YzEvZmJmMjQwNzM0ZmNlZDJjYWQ4YTUzN2NhYzc4ZDY3NTk4OTQ0MzZkMWYxMmExMDc5NDllY2EwYzQzMDYyMDkwNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=PKpyRjHbuN-CdkAx6%7E3vwgDYgA0-AycFEc0TJY3-q06iIcNSDoKTWHRvweUpmYJPsHXY1EgXAw1bP04MQ-ZcTdQVNjRtyOXDsnlOCKEqZEi2TToO6voDOOY%7EEmcYASYM2DyYaVwbb0WVJm7RXtbfHPwgWBfPcG1Ymvh6w9oJPdDSvKafgxR%7EzNVNew1Uj3BoqvdnLoZsVKjPnA3oYAAgjAobinxbgjKS9OXU5tJMpw4KewZikrOCbu6I5U9oB%7EI5vZgokRNsSDOloZVapmQXO7KbLoiQCsmQw4UIUMqror2d0ms46d8B9eMUKe7l2RcyayE3xWJGBztrJaXOniguew__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.163.189.20, 3.163.189.28, 3.163.189.91, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.163.189.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14098786 (13M) [binary/octet-stream]\n",
            "Saving to: ‘Data/ada/models/WD_0.pth’\n",
            "\n",
            "WD_0.pth            100%[===================>]  13.45M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-12-23 16:31:18 (162 MB/s) - ‘Data/ada/models/WD_0.pth’ saved [14098786/14098786]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 切分数据集\n",
        "\n",
        "!python3 audio_slicer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIEKmm7hVavz",
        "outputId": "ee9d7d89-932c-4fd3-e3e5-fb957ff59bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 转写和标注\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!python3 short_audio_transcribe.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qnSCKOlWpXx",
        "outputId": "19196bbb-6068-495b-a6da-94e865f3cf0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-it95ds57\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-it95ds57\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=981e4644a0cca9f3153fa190e95d0261dae8fb91cda09f01c777b61eaf94e592\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2fmn1atc/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n",
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:14<00:00, 105MiB/s]\n",
            "Data/ada/raw\n",
            "Detected language: en\n",
            "Louise and I had an arrangement.\n",
            "Processed: 1/27\n",
            "Detected language: en\n",
            "Leave the girl. She's lost no matter what.\n",
            "Processed: 2/27\n",
            "Detected language: en\n",
            "You can stop right there, Leon.\n",
            "Processed: 3/27\n",
            "Detected language: en\n",
            "We're changing course. Now.\n",
            "Processed: 4/27\n",
            "Detected language: en\n",
            "Sorry, nothing yet.\n",
            "Processed: 5/27\n",
            "Detected language: en\n",
            "How about we continue this discussion another time?\n",
            "Processed: 6/27\n",
            "Detected language: en\n",
            "But my little helper is creating\n",
            "Processed: 7/27\n",
            "Detected language: en\n",
            "The deal was, we get you out of here when you deliver the amber. No amber, no protection, Louise.\n",
            "Processed: 8/27\n",
            "Detected language: en\n",
            "You know I don't work and tell.\n",
            "Processed: 9/27\n",
            "Detected language: en\n",
            "wouldn't make me use this.\n",
            "Processed: 10/27\n",
            "Detected language: en\n",
            "Would you? You don't seem surprised.\n",
            "Processed: 11/27\n",
            "Detected language: en\n",
            "I do. The kind you like.\n",
            "Processed: 12/27\n",
            "Detected language: en\n",
            "Interesting.\n",
            "Processed: 13/27\n",
            "Detected language: en\n",
            "He's a good boy. Predictable.\n",
            "Processed: 14/27\n",
            "Detected language: en\n",
            "Not a bad move\n",
            "Processed: 15/27\n",
            "Detected language: en\n",
            "Don't worry, I'll take good care of it.\n",
            "Processed: 16/27\n",
            "Detected language: en\n",
            "Everything will work out just fine.\n",
            "Processed: 17/27\n",
            "Detected language: en\n",
            "You walk away now, and who knows?\n",
            "Processed: 18/27\n",
            "Detected language: en\n",
            "Now where's the amber?\n",
            "Processed: 19/27\n",
            "Detected language: en\n",
            "So, we're talking millions of casualties?\n",
            "Processed: 20/27\n",
            "Detected language: en\n",
            "What are you planning to do with this?\n",
            "Processed: 21/27\n",
            "Detected language: en\n",
            "Maybe you'll live to meet me again.\n",
            "Processed: 22/27\n",
            "Detected language: en\n",
            "Just one question.\n",
            "Processed: 23/27\n",
            "Detected language: en\n",
            "Nothing personal, Leon.\n",
            "Processed: 24/27\n",
            "Detected language: en\n",
            "Very smooth. Ah, Leon.\n",
            "Processed: 25/27\n",
            "Detected language: en\n",
            "Quite the commotion.\n",
            "Processed: 26/27\n",
            "Detected language: en\n",
            "And I might get you that greeting you were looking for.\n",
            "Processed: 27/27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 重新采样\n",
        "!python3 resample.py --sr 44100 --in_dir ./Data/ada/raw/ --out_dir ./Data/ada/wavs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifchw_XNbTk_",
        "outputId": "80f74312-ce9e-4f88-a6a4-c18b4396154d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27it [00:00, 61.34it/s]\n",
            "音频重采样完毕!\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 预处理标签文件\n",
        "!python3 preprocess_text.py --transcription-path ./Data/ada/esd.list --train-path ./Data/ada/train.list --val-path ./Data/ada/val.list --config-path ./Data/ada/configs/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTXlatMMbf2d",
        "outputId": "8239a01b-5749-466b-bf19-8811e96700f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch_model.bin: 100% 1.32G/1.32G [00:10<00:00, 122MB/s] \n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 115MB/s]\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "100% 27/27 [00:00<00:00, 4457.63it/s]\n",
            "总重复音频数：0，总未找到的音频数:0\n",
            "训练集和验证集生成完成！\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 生成 BERT 特征文件\n",
        "!python3 bert_gen.py --config-path ./Data/ada/configs/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCYwtjKZchFd",
        "outputId": "44218266-ee0c-4916-e85a-0d9c37c7f2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 27/27 [00:33<00:00,  1.25s/it]\n",
            "bert生成完毕!, 共有27个bert.pt生成!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 开始训练\n",
        "!python3 train_ms.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwXxlmS5deJ6",
        "outputId": "796b9766-502b-45b7-9e1b-3724264f09a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-23 16:38:20.173452: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-23 16:38:20.173554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-23 16:38:20.278510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-23 16:38:20.494836: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-23 16:38:22.435156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "加载config中的配置localhost\n",
            "加载config中的配置10086\n",
            "加载config中的配置1\n",
            "加载config中的配置0\n",
            "加载config中的配置0\n",
            "加载环境变量 \n",
            "MASTER_ADDR: localhost,\n",
            "MASTER_PORT: 10086,\n",
            "WORLD_SIZE: 1,\n",
            "RANK: 0,\n",
            "LOCAL_RANK: 0\n",
            "\u001b[32m12-23 16:38:30\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:62 | Init dataset...\n",
            "100% 23/23 [00:00<00:00, 31159.24it/s]\n",
            "\u001b[32m12-23 16:38:30\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:77 | skipped: 0, total: 23\n",
            "\u001b[32m12-23 16:38:30\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:62 | Init dataset...\n",
            "100% 4/4 [00:00<00:00, 29279.61it/s]\n",
            "\u001b[32m12-23 16:38:30\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:77 | skipped: 0, total: 4\n",
            "Using noise scaled MAS for VITS2\n",
            "Using duration discriminator for VITS2\n",
            "ERROR:models:cond.weight is not in the checkpoint\n",
            "INFO:models:Loaded checkpoint 'Data/ada/models/DUR_0.pth' (iteration 0)\n",
            "ERROR:models:enc_p.encoder.spk_emb_linear.weight is not in the checkpoint\n",
            "ERROR:models:dec.cond.weight is not in the checkpoint\n",
            "ERROR:models:enc_q.enc.cond_layer.weight_v is not in the checkpoint\n",
            "ERROR:models:flow.flows.0.enc.spk_emb_linear.weight is not in the checkpoint\n",
            "ERROR:models:flow.flows.2.enc.spk_emb_linear.weight is not in the checkpoint\n",
            "ERROR:models:flow.flows.4.enc.spk_emb_linear.weight is not in the checkpoint\n",
            "ERROR:models:flow.flows.6.enc.spk_emb_linear.weight is not in the checkpoint\n",
            "ERROR:models:sdp.cond.weight is not in the checkpoint\n",
            "ERROR:models:dp.cond.weight is not in the checkpoint\n",
            "ERROR:models:emb_g.weight is not in the checkpoint\n",
            "INFO:models:Loaded checkpoint 'Data/ada/models/G_0.pth' (iteration 0)\n",
            "INFO:models:Loaded checkpoint 'Data/ada/models/D_0.pth' (iteration 0)\n",
            "******************检测到模型存在，epoch为 1，gloabl step为 0*********************\n",
            "INFO:models:Loaded checkpoint 'Data/ada/models/WD_0.pth' (iteration 0)\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0% 0/6 [00:00<?, ?it/s]INFO:models:Train Epoch: 0 [0%]\n",
            "INFO:models:[2.7351865768432617, 2.4700968265533447, 6.226019859313965, 23.852706909179688, 4.0050578117370605, 5.843006610870361, 0, 0.00019999000000000001]\n",
            "Evaluating ...\n",
            "INFO:models:Saving model and optimizer state at iteration 0 to Data/ada/models/G_0.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 0 to Data/ada/models/D_0.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 0 to Data/ada/models/WD_0.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 0 to Data/ada/models/DUR_0.pth\n",
            "100% 6/6 [00:27<00:00,  4.56s/it]\n",
            "INFO:models:====> Epoch: 0\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:08<00:00,  1.48s/it]\n",
            "INFO:models:====> Epoch: 1\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:08<00:00,  1.46s/it]\n",
            "INFO:models:====> Epoch: 2\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.53s/it]\n",
            "INFO:models:====> Epoch: 3\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.53s/it]\n",
            "INFO:models:====> Epoch: 4\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.57s/it]\n",
            "INFO:models:====> Epoch: 5\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.53s/it]\n",
            "INFO:models:====> Epoch: 6\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:08<00:00,  1.49s/it]\n",
            "INFO:models:====> Epoch: 7\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 33% 2/6 [00:03<00:06,  1.58s/it]INFO:models:Train Epoch: 8 [33%]\n",
            "INFO:models:[2.829066276550293, 2.0448553562164307, 5.720618724822998, 20.82781219482422, 2.6438045501708984, 1.6997442245483398, 50, 0.0001999200139986001]\n",
            "Evaluating ...\n",
            "INFO:models:Saving model and optimizer state at iteration 8 to Data/ada/models/G_50.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 8 to Data/ada/models/D_50.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 8 to Data/ada/models/WD_50.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 8 to Data/ada/models/DUR_50.pth\n",
            "100% 6/6 [00:21<00:00,  3.54s/it]\n",
            "INFO:models:====> Epoch: 8\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.55s/it]\n",
            "INFO:models:====> Epoch: 9\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.56s/it]\n",
            "INFO:models:====> Epoch: 10\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.56s/it]\n",
            "INFO:models:====> Epoch: 11\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.52s/it]\n",
            "INFO:models:====> Epoch: 12\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:08<00:00,  1.49s/it]\n",
            "INFO:models:====> Epoch: 13\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.53s/it]\n",
            "INFO:models:====> Epoch: 14\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.56s/it]\n",
            "INFO:models:====> Epoch: 15\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 67% 4/6 [00:06<00:03,  1.56s/it]INFO:models:Train Epoch: 16 [67%]\n",
            "INFO:models:[2.4305906295776367, 2.25631046295166, 6.707864284515381, 22.08708953857422, 3.1340713500976562, 2.869493246078491, 100, 0.00019984005998600234]\n",
            "Evaluating ...\n",
            "INFO:models:Saving model and optimizer state at iteration 16 to Data/ada/models/G_100.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 16 to Data/ada/models/D_100.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 16 to Data/ada/models/WD_100.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 16 to Data/ada/models/DUR_100.pth\n",
            "100% 6/6 [00:19<00:00,  3.30s/it]\n",
            "INFO:models:====> Epoch: 16\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.53s/it]\n",
            "INFO:models:====> Epoch: 17\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.55s/it]\n",
            "INFO:models:====> Epoch: 18\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.60s/it]\n",
            "INFO:models:====> Epoch: 19\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.58s/it]\n",
            "INFO:models:====> Epoch: 20\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.56s/it]\n",
            "INFO:models:====> Epoch: 21\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.56s/it]\n",
            "INFO:models:====> Epoch: 22\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.54s/it]\n",
            "INFO:models:====> Epoch: 23\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:09<00:00,  1.52s/it]\n",
            "INFO:models:====> Epoch: 24\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0% 0/6 [00:00<?, ?it/s]INFO:models:Train Epoch: 25 [0%]\n",
            "INFO:models:[2.5526604652404785, 2.175715684890747, 6.805540561676025, 20.01245880126953, 2.5321855545043945, 1.5644482374191284, 150, 0.0001997501499425159]\n",
            "Evaluating ...\n",
            "INFO:models:Saving model and optimizer state at iteration 25 to Data/ada/models/G_150.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 25 to Data/ada/models/D_150.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 25 to Data/ada/models/WD_150.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 25 to Data/ada/models/DUR_150.pth\n",
            "100% 6/6 [00:19<00:00,  3.33s/it]\n",
            "INFO:models:====> Epoch: 25\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 67% 4/6 [00:06<00:03,  1.66s/it]\n",
            "Error in sys.excepthook:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/exceptiongroup/_formatting.py\", line 68, in exceptiongroup_excepthook\n",
            "    def exceptiongroup_excepthook(\n",
            "KeyboardInterrupt\n",
            "\n",
            "Original exception was:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Bert-vits2-V2.3/train_ms.py\", line 838, in <module>\n",
            "    run()\n",
            "  File \"/content/Bert-vits2-V2.3/train_ms.py\", line 354, in run\n",
            "    train_and_evaluate(\n",
            "  File \"/content/Bert-vits2-V2.3/train_ms.py\", line 468, in train_and_evaluate\n",
            "    ) = net_g(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1519, in forward\n",
            "    else self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1355, in _run_ddp_forward\n",
            "    return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Bert-vits2-V2.3/models.py\", line 960, in forward\n",
            "    z_p = self.flow(z, y_mask, g=g)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Bert-vits2-V2.3/models.py\", line 143, in forward\n",
            "    x, _ = flow(x, x_mask, g=g, reverse=reverse)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Bert-vits2-V2.3/modules.py\", line 564, in forward\n",
            "    h = self.enc(h, x_mask, g=g)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Bert-vits2-V2.3/attentions.py\", line 113, in forward\n",
            "    y = self.drop(y)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "KeyboardInterrupt\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 开始推理\n",
        "!python3 webui.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgccXqt2fZyE",
        "outputId": "8db327e9-3ba6-4822-d927-a0c41a55b2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| numexpr.utils | INFO | NumExpr defaulting to 2 threads.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "| utils | INFO | Loaded checkpoint 'Data/ada/models/G_150.pth' (iteration 25)\n",
            "推理页面已开启!\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://814833a6f477ba151c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py:183: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2361, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Bert-vits2-V2.3/webui.py\", line 552, in <module>\n",
            "    app.launch(share=True, server_port=config.webui_config.port)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2266, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2365, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/networking.py\", line 75, in close\n",
            "    self.thread.join()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://814833a6f477ba151c.gradio.live\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ]
}